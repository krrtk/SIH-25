{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9504dbc3-4e81-4463-bad0-15b424a1ed7b",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c1d669-971a-4169-a854-31e175d0d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Gradient Boosting model...\n",
      "Model training completed.\n",
      "Model saved successfully to phishing_gradient_boosting_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "def train_and_save_model(dataset_path='dataset.csv', model_output_path='phishing_gradient_boosting_model.joblib'):\n",
    "    \"\"\"\n",
    "    Loads the dataset, trains the Gradient Boosting model, and saves it to a file.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {dataset_path} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Prepare data\n",
    "    if 'index' in df.columns:\n",
    "        df = df.drop('index', axis=1)\n",
    "    X = df.drop('Result', axis=1)\n",
    "    y = df['Result']\n",
    "\n",
    "    # --- MODEL CHANGE ---\n",
    "    # We've replaced RandomForestClassifier with GradientBoostingClassifier\n",
    "    # and given it stronger starting parameters.\n",
    "    print(\"Training the Gradient Boosting model...\")\n",
    "    model = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    \n",
    "    # Using .values is not strictly necessary for newer scikit-learn versions but is safe\n",
    "    model.fit(X.values, y)\n",
    "    print(\"Model training completed.\")\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(model, model_output_path)\n",
    "    print(f\"Model saved successfully to {model_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a01b474-a450-4510-9abb-3dba646cff15",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654cb3f-d226-45c6-8e9b-f97af7f8e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing URL Detector is ready. Enter a URL to check.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://www.youtube.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://www.youtube.com/\n",
      "Overall URL risk classification. Clean URL - SAFE\n",
      "Suspicious Activity. No Suspicious Activity\n",
      "Domain. www.youtube.com\n",
      "IP Address. 142.251.223.142\n",
      "Malware. No Malware Issues\n",
      "Phishing. No Phishing Issues\n",
      "Risk Score. 0 - Clean\n",
      "Parked Domain. Not Parked\n",
      "Spamming Domain. No SPAM Issues\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. 20 years ago\n",
      "HTTP Status Code. 200\n",
      "Page Size. 667486 bytes\n",
      "Web Server. ESF\n",
      "Content Type. text/html\n",
      "Free Hosted Content. False\n",
      "SPF/DMARC Record. Not found\n",
      "DNS A Records. 142.250.193.46, 142.250.67.78, 142.251.223.142, 216.58.200.174, 216.58.196.110, 172.217.27.174, 142.250.182.110, 172.217.167.206, 172.217.166.14, 172.217.26.110, 142.250.193.14, 142.250.183.238, 142.250.182.46, 172.217.26.14, 142.250.193.142, 172.217.26.46\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Safe\n",
      "Decision Based On: 30-Feature Model Prediction\n",
      "Model's Highest Confidence (from 30 features): 99.93%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  0.1% chance of being -1 (Unsafe (Phishing))\n",
      "  99.9% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://lms.vit.ac.in/login/index.php\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://lms.vit.ac.in/login/index.php\n",
      "Overall URL risk classification. Unreachable\n",
      "Suspicious Activity. URL Unreachable\n",
      "Domain. lms.vit.ac.in\n",
      "IP Address. N/A\n",
      "Malware. N/A\n",
      "Phishing. N/A\n",
      "Risk Score. N/A\n",
      "Parked Domain. N/A\n",
      "Spamming Domain. N/A\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. N/A\n",
      "HTTP Status Code. N/A\n",
      "Page Size. N/A\n",
      "Web Server. N/A\n",
      "Content Type. N/A\n",
      "Free Hosted Content. N/A\n",
      "SPF/DMARC Record. N/A\n",
      "DNS A Records. N/A\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Unsafe (URL Unreachable)\n",
      "Decision Based On: URL Unreachable\n",
      "Model's Highest Confidence (from 30 features): 100.00%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  100.0% chance of being -1 (Unsafe (Phishing))\n",
      "  0.0% chance of being 0 (Neutral)\n",
      "  0.0% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://ahcswh.com/v2/check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://ahcswh.com/v2/check\n",
      "Overall URL risk classification. Clean URL - SAFE\n",
      "Suspicious Activity. No Suspicious Activity\n",
      "Domain. ahcswh.com\n",
      "IP Address. 43.165.132.68\n",
      "Malware. No Malware Issues\n",
      "Phishing. No Phishing Issues\n",
      "Risk Score. 0 - Clean\n",
      "Parked Domain. Not Parked\n",
      "Spamming Domain. No SPAM Issues\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. 347 days ago\n",
      "HTTP Status Code. 200\n",
      "Page Size. 1621 bytes\n",
      "Web Server. nginx/1.26.3\n",
      "Content Type. text/html\n",
      "Free Hosted Content. False\n",
      "SPF/DMARC Record. Not found\n",
      "DNS A Records. 43.165.132.68\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Safe\n",
      "Decision Based On: 30-Feature Model Prediction\n",
      "Model's Highest Confidence (from 30 features): 99.92%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  0.1% chance of being -1 (Unsafe (Phishing))\n",
      "  99.9% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://github.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://github.com/\n",
      "Overall URL risk classification. Suspicious\n",
      "Suspicious Activity. Suspicious Keywords Detected\n",
      "Domain. github.com\n",
      "IP Address. 20.207.73.82\n",
      "Malware. No Malware Issues\n",
      "Phishing. Phishing Suspected\n",
      "Risk Score. 0 - Suspicious\n",
      "Parked Domain. Not Parked\n",
      "Spamming Domain. No SPAM Issues\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. 17 years ago\n",
      "HTTP Status Code. 200\n",
      "Page Size. 560602 bytes\n",
      "Web Server. github.com\n",
      "Content Type. text/html\n",
      "Free Hosted Content. False\n",
      "SPF/DMARC Record. SPF and DMARC Found\n",
      "DNS A Records. 20.207.73.82\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Unsafe (Phishing - Content Alert!)\n",
      "Decision Based On: Content-Based Feature Override\n",
      "Model's Highest Confidence (from 30 features): 99.94%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  0.1% chance of being -1 (Unsafe (Phishing))\n",
      "  99.9% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://baggio130.tithelysetup.com/pitcho\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://baggio130.tithelysetup.com/pitcho\n",
      "Overall URL risk classification. Suspicious\n",
      "Suspicious Activity. Neutral Model Indication\n",
      "Domain. baggio130.tithelysetup.com\n",
      "IP Address. 52.35.132.113\n",
      "Malware. No Malware Issues\n",
      "Phishing. Possibly benign, check manually\n",
      "Risk Score. 1 - Suspicious\n",
      "Parked Domain. Not Parked\n",
      "Spamming Domain. No SPAM Issues\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. 6 years ago\n",
      "HTTP Status Code. N/A\n",
      "Page Size. N/A\n",
      "Web Server. N/A\n",
      "Content Type. N/A\n",
      "Free Hosted Content. False\n",
      "SPF/DMARC Record. Not found\n",
      "DNS A Records. 52.35.132.113\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Neutral (Content Minor Suspicion)\n",
      "Decision Based On: 30-Feature Model Prediction\n",
      "Model's Highest Confidence (from 30 features): 98.34%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  1.7% chance of being -1 (Unsafe (Phishing))\n",
      "  98.3% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://lombardi12.kinsta.cloud/patcho\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://lombardi12.kinsta.cloud/patcho\n",
      "Overall URL risk classification. Unreachable\n",
      "Suspicious Activity. URL Unreachable\n",
      "Domain. lombardi12.kinsta.cloud\n",
      "IP Address. N/A\n",
      "Malware. N/A\n",
      "Phishing. N/A\n",
      "Risk Score. N/A\n",
      "Parked Domain. N/A\n",
      "Spamming Domain. N/A\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. N/A\n",
      "HTTP Status Code. N/A\n",
      "Page Size. N/A\n",
      "Web Server. N/A\n",
      "Content Type. N/A\n",
      "Free Hosted Content. N/A\n",
      "SPF/DMARC Record. N/A\n",
      "DNS A Records. N/A\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Unsafe (URL Unreachable)\n",
      "Decision Based On: URL Unreachable\n",
      "Model's Highest Confidence (from 30 features): 100.00%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  100.0% chance of being -1 (Unsafe (Phishing))\n",
      "  0.0% chance of being 0 (Neutral)\n",
      "  0.0% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a URL (or type 'exit' to quit):  https://vtopcc.vit.ac.in/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "URL Scanning Details for https://vtopcc.vit.ac.in/\n",
      "Overall URL risk classification. Unreachable\n",
      "Suspicious Activity. URL Unreachable\n",
      "Domain. vtopcc.vit.ac.in\n",
      "IP Address. N/A\n",
      "Malware. N/A\n",
      "Phishing. N/A\n",
      "Risk Score. N/A\n",
      "Parked Domain. N/A\n",
      "Spamming Domain. N/A\n",
      "Domain Trust Rating. Login to View\n",
      "Domain Age. N/A\n",
      "HTTP Status Code. N/A\n",
      "Page Size. N/A\n",
      "Web Server. N/A\n",
      "Content Type. N/A\n",
      "Free Hosted Content. N/A\n",
      "SPF/DMARC Record. N/A\n",
      "DNS A Records. N/A\n",
      "\n",
      "--- Model's Internal Details ---\n",
      "Model's Final Decision: Unsafe (URL Unreachable)\n",
      "Decision Based On: URL Unreachable\n",
      "Model's Highest Confidence (from 30 features): 100.00%\n",
      "Model's Individual Class Probabilities (from 30 features):\n",
      "  100.0% chance of being -1 (Unsafe (Phishing))\n",
      "  0.0% chance of being 0 (Neutral)\n",
      "  0.0% chance of being 1 (Safe)\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import whois\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import dns.resolver\n",
    "import re\n",
    "import sys\n",
    "import numpy as np # Needed for array operations, especially with probabilities\n",
    "from collections import defaultdict # For easier data collection\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_domain(url):\n",
    "    try:\n",
    "        return urllib.parse.urlparse(url).netloc\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# === NEW FUNCTION TO CHECK IF URL IS ACCESSIBLE ===\n",
    "# ==============================================================================\n",
    "def is_url_accessible(url):\n",
    "    \"\"\"\n",
    "    Checks if a URL is reachable by sending a HEAD request.\n",
    "    Returns True if the URL responds with a successful status code, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a HEAD request to get only headers, which is faster.\n",
    "        # Set a timeout to avoid waiting indefinitely.\n",
    "        # Add a common User-Agent to avoid being blocked by some servers.\n",
    "        response = requests.head(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        \n",
    "        # Check for a successful status code (e.g., 200 OK)\n",
    "        # We consider any 4xx (client error) or 5xx (server error) as inaccessible.\n",
    "        if response.status_code < 400:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.exceptions.RequestException:\n",
    "        # Catches connection errors, timeouts, etc.\n",
    "        return False\n",
    "\n",
    "# Modified to use User-Agent\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# === ADDITIONAL HELPER FUNCTIONS FOR DETAILED REPORTING ===\n",
    "# ==============================================================================\n",
    "\n",
    "def get_ip_address(domain):\n",
    "    \"\"\"Resolves a domain name to its primary IPv4 address.\"\"\"\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain, 'A')\n",
    "        return answers[0].address\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.exception.Timeout):\n",
    "        return \"N/A\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_all_ip_addresses(domain):\n",
    "    \"\"\"Resolves a domain name to all its IPv4 addresses.\"\"\"\n",
    "    ips = []\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain, 'A')\n",
    "        for rdata in answers:\n",
    "            ips.append(rdata.address)\n",
    "        return ips\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.exception.Timeout):\n",
    "        return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_http_details(url):\n",
    "    \"\"\"Fetches HTTP status code, page size, web server, and content type.\"\"\"\n",
    "    details = {\n",
    "        'HTTP Status Code': 'N/A',\n",
    "        'Page Size': 'N/A',\n",
    "        'Web Server': 'N/A',\n",
    "        'Content Type': 'N/A',\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status() # Raise an exception for HTTP errors\n",
    "        \n",
    "        details['HTTP Status Code'] = response.status_code\n",
    "        details['Page Size'] = len(response.content) # Page size in bytes\n",
    "        details['Web Server'] = response.headers.get('Server', 'N/A')\n",
    "        details['Content Type'] = response.headers.get('Content-Type', 'N/A').split(';')[0] # Remove charset info\n",
    "        \n",
    "    except requests.exceptions.RequestException:\n",
    "        pass # Keep N/A for connection/HTTP errors\n",
    "    return details\n",
    "\n",
    "def get_whois_domain_age(domain):\n",
    "    \"\"\"Extracts creation date and calculates age from WHOIS.\"\"\"\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        if w.creation_date:\n",
    "            cre_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
    "            if isinstance(cre_date, datetime):\n",
    "                age_delta = datetime.now() - cre_date\n",
    "                years = age_delta.days // 365\n",
    "                if years == 0:\n",
    "                    return f\"{age_delta.days} days ago\"\n",
    "                elif years == 1:\n",
    "                    return \"1 year ago\"\n",
    "                else:\n",
    "                    return f\"{years} years ago\"\n",
    "        return \"N/A\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "def check_spf_dmarc(domain):\n",
    "    \"\"\"Checks for SPF and DMARC DNS records.\"\"\"\n",
    "    spf_found = False\n",
    "    dmarc_found = False\n",
    "    \n",
    "    try:\n",
    "        # Check SPF records (TXT records starting with \"v=spf1\")\n",
    "        txt_records = dns.resolver.resolve(domain, 'TXT')\n",
    "        for rdata in txt_records:\n",
    "            for txt_string in rdata.strings:\n",
    "                if b\"v=spf1\" in txt_string.lower():\n",
    "                    spf_found = True\n",
    "                    break\n",
    "            if spf_found: break\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.exception.Timeout):\n",
    "        pass # No TXT records or domain doesn't exist\n",
    "\n",
    "    try:\n",
    "        # Check DMARC record (_dmarc.domain TXT record)\n",
    "        dmarc_records = dns.resolver.resolve(f\"_dmarc.{domain}\", 'TXT')\n",
    "        for rdata in dmarc_records:\n",
    "            for txt_string in rdata.strings:\n",
    "                if b\"v=dmarc1\" in txt_string.lower():\n",
    "                    dmarc_found = True\n",
    "                    break\n",
    "            if dmarc_found: break\n",
    "    except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN, dns.exception.Timeout):\n",
    "        pass # No DMARC record or domain doesn't exist\n",
    "    \n",
    "    if spf_found and dmarc_found:\n",
    "        return \"SPF and DMARC Found\"\n",
    "    elif spf_found:\n",
    "        return \"SPF Found\"\n",
    "    elif dmarc_found:\n",
    "        return \"DMARC Found\"\n",
    "    else:\n",
    "        return \"Not found\"\n",
    "\n",
    "# Placeholder for Free Hosted Content - requires more complex checks or a database\n",
    "def is_free_hosted(domain):\n",
    "    # This is a complex check, often involving lists of known free hosting providers.\n",
    "    # For now, we'll return a placeholder.\n",
    "    # Example: if \"blogspot.com\" in domain or \"wixsite.com\" in domain etc.\n",
    "    free_hosts = [\"blogspot.com\", \"wixsite.com\", \"weebly.com\", \"github.io\", \"netlify.app\", \"surge.sh\"]\n",
    "    if any(fh in domain for fh in free_hosts):\n",
    "        return True\n",
    "    return False # Default to false, as a comprehensive check is hard without external data\n",
    "\n",
    "# ==============================================================================\n",
    "# === FEATURE EXTRACTION FUNCTIONS (UNCHANGED) ===\n",
    "# ==============================================================================\n",
    "\n",
    "def having_ip_address(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        if re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\", domain): return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def url_length(url):\n",
    "    if len(url) < 54: return 1\n",
    "    elif 54 <= len(url) <= 75: return 0\n",
    "    else: return -1\n",
    "\n",
    "def shortening_service(url):\n",
    "    shortening_services = [\"bit.ly\", \"goo.gl\", \"t.co\", \"tinyurl.com\", \"is.gd\", \"cli.gs\", \"tr.im\", \"ow.ly\", \"tiny.cc\"]\n",
    "    domain = get_domain(url)\n",
    "    if domain in shortening_services: return -1\n",
    "    return 1\n",
    "\n",
    "def having_at_symbol(url):\n",
    "    if \"@\" in url: return -1\n",
    "    return 1\n",
    "\n",
    "def double_slash_redirecting(url):\n",
    "    path = urllib.parse.urlparse(url).path\n",
    "    if \"//\" in path and path.find(\"//\") > 0: # More precise to catch internal //\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "def prefix_suffix(url):\n",
    "    if \"-\" in get_domain(url): return -1\n",
    "    return 1\n",
    "\n",
    "def having_sub_domain(url):\n",
    "    dots = get_domain(url).count('.')\n",
    "    if dots == 2: return 0\n",
    "    elif dots > 2: return -1\n",
    "    return 1\n",
    "\n",
    "def ssl_final_state(url):\n",
    "    try:\n",
    "        if url.startswith(\"https\"): return 1\n",
    "        return -1\n",
    "    except: return -1\n",
    "\n",
    "def domain_registration_length(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if w.expiration_date and w.creation_date:\n",
    "            exp_date = w.expiration_date[0] if isinstance(w.expiration_date, list) else w.expiration_date\n",
    "            cre_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
    "            if exp_date and cre_date:\n",
    "                if (exp_date - cre_date).days / 365 <= 1: return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def age_of_domain(url): # This function now just returns -1/1 for model, not display string\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if w.creation_date:\n",
    "            cre_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
    "            if (datetime.now() - cre_date).days < 180: return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def dns_record(url):\n",
    "    try:\n",
    "        dns.resolver.resolve(get_domain(url), 'A'); return 1\n",
    "    except: return -1\n",
    "\n",
    "def abnormal_url(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if domain.lower() not in str(w).lower(): return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def web_traffic(url): return 0\n",
    "def page_rank(url): return 0\n",
    "def google_index(url): return 1\n",
    "def links_pointing_to_page(url): return 0\n",
    "def statistical_report(url): return 1\n",
    "def favicon(url): return 1\n",
    "def port(url): return 1\n",
    "def https_token(url): return 1\n",
    "\n",
    "def request_url(url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return -1\n",
    "    domain = get_domain(url)\n",
    "    image_count = 0\n",
    "    external_image_count = 0\n",
    "    for img in soup.find_all('img'):\n",
    "        if img.has_attr('src'):\n",
    "            image_count += 1\n",
    "            src_domain = get_domain(urllib.parse.urljoin(url, img['src']))\n",
    "            if src_domain != domain:\n",
    "                external_image_count += 1\n",
    "    if image_count == 0: return 1\n",
    "    percentage = (external_image_count / image_count) * 100\n",
    "    if percentage < 22.0: return 1\n",
    "    elif 22.0 <= percentage < 61.0: return 0\n",
    "    else: return -1\n",
    "\n",
    "def url_of_anchor(url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return -1\n",
    "    domain = get_domain(url)\n",
    "    anchor_count = 0\n",
    "    external_anchor_count = 0\n",
    "    for a in soup.find_all('a'):\n",
    "        if a.has_attr('href'):\n",
    "            anchor_count += 1\n",
    "            href = a['href']\n",
    "            if href.startswith('#') or href.startswith('mailto:') or 'javascript:void(0)' in href.lower():\n",
    "                anchor_count -= 1\n",
    "                continue\n",
    "            href_domain = get_domain(urllib.parse.urljoin(url, href))\n",
    "            if href_domain != domain:\n",
    "                external_anchor_count += 1\n",
    "    if anchor_count == 0: return 1\n",
    "    percentage = (external_anchor_count / anchor_count) * 100\n",
    "    if percentage < 31.0: return 1\n",
    "    elif 31.0 <= percentage < 67.0: return 0\n",
    "    else: return -1\n",
    "\n",
    "def links_in_tags(url): # Placeholder for this feature (adjust if you have real logic)\n",
    "    # This feature usually involves examining link distribution in HTML tags like <A>, <Link>, <Meta>\n",
    "    # For now, a neutral score.\n",
    "    return 0 \n",
    "\n",
    "def sfh(url): # Placeholder for SFH (Server Form Handler)\n",
    "    # Checks if form action is to an external domain or looks suspicious\n",
    "    # Requires parsing forms and their actions. For now, neutral.\n",
    "    return 0\n",
    "\n",
    "def submitting_to_email(url): # Placeholder\n",
    "    # Checks for mailto: links in form actions.\n",
    "    return 1\n",
    "\n",
    "def redirect(url): # Placeholder\n",
    "    # Checks for multiple redirections. requests library handles this internally.\n",
    "    return 1\n",
    "\n",
    "def on_mouseover(url): # Placeholder\n",
    "    # Checks for status bar changes on mouseover (JS based). Hard to detect without browser.\n",
    "    return 1\n",
    "\n",
    "def right_click(url): # Placeholder\n",
    "    # Checks for disabled right-click. Hard to detect without browser.\n",
    "    return 1\n",
    "\n",
    "def popup_window(url): # Placeholder\n",
    "    # Checks for pop-up windows. Hard to detect without browser.\n",
    "    return 1\n",
    "\n",
    "def iframe(url): # Placeholder\n",
    "    # Checks for iframes.\n",
    "    soup = get_soup(url)\n",
    "    if soup and soup.find('iframe'):\n",
    "        return -1 # Presence of iframe can sometimes be suspicious\n",
    "    return 1\n",
    "\n",
    "\n",
    "# --- NEW CONTENT-BASED FEATURES (for rule-based enhancement) ---\n",
    "\n",
    "def has_suspicious_keywords(url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return 0\n",
    "    suspicious_words = [\"login\", \"password\", \"verify\", \"account\", \"update\", \"security\", \"confirm\", \"bank\", \"credit card\", \"paypal\", \"alert\", \"urgent\"]\n",
    "    page_text = soup.get_text().lower()\n",
    "    found_count = sum(1 for word in suspicious_words if word in page_text)\n",
    "    if found_count >= 3: return -1\n",
    "    elif found_count >= 1: return 0\n",
    "    else: return 1\n",
    "\n",
    "def has_login_form(url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return 0\n",
    "    forms = soup.find_all('form')\n",
    "    if not forms: return 1\n",
    "    for form in forms:\n",
    "        password_input = form.find('input', {'type': 'password'})\n",
    "        email_input = form.find('input', {'type': 'email'})\n",
    "        text_input_name = form.find('input', {'type': 'text', 'name': re.compile(r'user|login|account|email|username', re.I)})\n",
    "        if password_input or email_input or text_input_name:\n",
    "            action = form.get('action')\n",
    "            if action:\n",
    "                action_domain = get_domain(urllib.parse.urljoin(url, action))\n",
    "                if action_domain != get_domain(url) or not action_domain: return -1\n",
    "            else: return -1\n",
    "    return 1\n",
    "\n",
    "def external_link_ratio(url):\n",
    "    soup = get_soup(url)\n",
    "    if not soup: return 0\n",
    "    main_domain = get_domain(url)\n",
    "    total_links = 0\n",
    "    external_domains = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.startswith('#') or href.startswith('mailto:') or 'javascript:void(0)' in href.lower(): continue\n",
    "        full_href = urllib.parse.urljoin(url, href)\n",
    "        link_domain = get_domain(full_href)\n",
    "        if link_domain and link_domain != main_domain: external_domains.add(link_domain)\n",
    "        total_links += 1\n",
    "    if total_links == 0: return 1\n",
    "    ratio = len(external_domains) / total_links\n",
    "    if ratio > 0.4: return -1\n",
    "    elif ratio > 0.1: return 0\n",
    "    else: return 1\n",
    "\n",
    "\n",
    "# --- Define the order of your original 30 features ---\n",
    "# This order MUST match the order of features your model was trained on from dataset.csv\n",
    "ORIGINAL_30_FEATURES_ORDER = [\n",
    "    having_ip_address, url_length, shortening_service, having_at_symbol,\n",
    "    double_slash_redirecting, prefix_suffix, having_sub_domain, ssl_final_state,\n",
    "    domain_registration_length, favicon, port, https_token, request_url,\n",
    "    url_of_anchor, links_in_tags, sfh, submitting_to_email, abnormal_url,\n",
    "    redirect, on_mouseover, right_click, popup_window, iframe,\n",
    "    age_of_domain, dns_record, web_traffic, page_rank, google_index,\n",
    "    links_pointing_to_page, statistical_report\n",
    "]\n",
    "\n",
    "# --- Define the 3 ADDITIONAL content-based features for rule-based override ---\n",
    "ADDITIONAL_3_FEATURES = [\n",
    "    has_suspicious_keywords, has_login_form, external_link_ratio\n",
    "]\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# === MODIFIED PREDICTION LOGIC WITH 30-FEATURE MODEL + RULE-BASED OVERRIDE ===\n",
    "# =================================================================\n",
    "def predict_url_with_enhancement(model_30_features, url, safe_confidence_threshold=0.94):\n",
    "    \"\"\"\n",
    "    Predicts the class of a URL using a 30-feature model, then enhances the decision\n",
    "    with 3 additional content-based features through a rule-based system.\n",
    "    Also collects comprehensive URL details for a structured report.\n",
    "    \"\"\"\n",
    "    \n",
    "    report_details = defaultdict(lambda: 'N/A')\n",
    "    report_details['Domain'] = get_domain(url)\n",
    "    report_details['Full URL'] = url\n",
    "\n",
    "    # --- Step 1: Accessibility Check ---\n",
    "    if not is_url_accessible(url):\n",
    "        prob_dist = {-1: 1.0, 0: 0.0, 1: 0.0}\n",
    "        report_details['Overall URL risk classification'] = \"Unreachable\"\n",
    "        report_details['Suspicious Activity'] = \"URL Unreachable\"\n",
    "        return \"Unsafe (URL Unreachable)\", prob_dist, 1.0, \"URL Unreachable\", report_details\n",
    "\n",
    "    # --- Collect Additional Report Details Proactively ---\n",
    "    # These calls might be redundant if the feature functions already do this,\n",
    "    # but ensures they are captured for the report.\n",
    "    domain = get_domain(url)\n",
    "    \n",
    "    report_details['IP Address'] = get_ip_address(domain)\n",
    "    report_details['DNS A Records'] = \", \".join(get_all_ip_addresses(domain)) if get_all_ip_addresses(domain) else \"Not found\"\n",
    "    \n",
    "    http_details = get_http_details(url)\n",
    "    report_details['HTTP Status Code'] = http_details['HTTP Status Code']\n",
    "    report_details['Page Size'] = f\"{http_details['Page Size']} bytes\" if http_details['Page Size'] != 'N/A' else 'N/A'\n",
    "    report_details['Web Server'] = http_details['Web Server']\n",
    "    report_details['Content Type'] = http_details['Content Type']\n",
    "    \n",
    "    report_details['Domain Age'] = get_whois_domain_age(domain)\n",
    "    report_details['Free Hosted Content'] = is_free_hosted(domain)\n",
    "    report_details['SPF/DMARC Record'] = check_spf_dmarc(domain)\n",
    "\n",
    "\n",
    "    # --- Step 2: Extract ORIGINAL 30 Features for the model ---\n",
    "    features_30 = [func(url) for func in ORIGINAL_30_FEATURES_ORDER]\n",
    "    \n",
    "    initial_label = \"Unknown\"\n",
    "    confidence = 0.0\n",
    "    prob_dist = {-1: 0.0, 0: 0.0, 1: 0.0} # Default for error case\n",
    "\n",
    "    try:\n",
    "        features_array_30 = np.array(features_30).reshape(1, -1)\n",
    "        \n",
    "        # Get prediction and probabilities from the 30-feature model\n",
    "        prediction_30 = model_30_features.predict(features_array_30)[0]\n",
    "        probabilities_array_30 = model_30_features.predict_proba(features_array_30)[0]\n",
    "        \n",
    "        prob_dist = {model_30_features.classes_[i]: probabilities_array_30[i] \n",
    "                     for i in range(len(model_30_features.classes_))}\n",
    "        confidence = max(probabilities_array_30)\n",
    "        \n",
    "        label_map = {1: \"Safe\", 0: \"Neutral\", -1: \"Unsafe (Phishing)\"}\n",
    "        initial_label = label_map.get(prediction_30, \"Unknown\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during 30-feature model prediction: {e}\", file=sys.stderr)\n",
    "        report_details['Overall URL risk classification'] = \"Error\"\n",
    "        return f\"Error: {e}\", {-1: 0.0, 0: 0.0, 1: 0.0}, 0.0, \"Model Error\", report_details\n",
    "\n",
    "    # --- Step 3: Apply Conservative Confidence Threshold on initial prediction ---\n",
    "    final_label = initial_label\n",
    "    decision_reason = \"30-Feature Model Prediction\"\n",
    "    classification_status = initial_label # For overall status\n",
    "\n",
    "    if initial_label == \"Safe\" and confidence < safe_confidence_threshold:\n",
    "        final_label = \"Unsafe (Phishing - Low Confidence Safety)\"\n",
    "        decision_reason = \"Conservative Confidence Threshold\"\n",
    "        classification_status = \"Suspicious\"\n",
    "\n",
    "    # --- Step 4: Extract and use the 3 ADDITIONAL features for further override ---\n",
    "    # This section gets executed AFTER the 30-feature model's prediction and initial thresholding.\n",
    "    \n",
    "    # Only if the current decision is not already \"Unsafe\" (or an error),\n",
    "    # do we check the additional features to potentially override to \"Unsafe\".\n",
    "    if \"Unsafe\" not in final_label and \"Error\" not in final_label:\n",
    "        \n",
    "        keyword_score = has_suspicious_keywords(url)\n",
    "        login_form_score = has_login_form(url)\n",
    "        external_link_ratio_score = external_link_ratio(url)\n",
    "        \n",
    "        suspicious_activity_reasons = []\n",
    "\n",
    "        if keyword_score == -1: suspicious_activity_reasons.append(\"Suspicious Keywords Detected\")\n",
    "        if login_form_score == -1: suspicious_activity_reasons.append(\"Login Form Detected\")\n",
    "        if external_link_ratio_score == -1: suspicious_activity_reasons.append(\"High External Link Ratio\")\n",
    "\n",
    "        if suspicious_activity_reasons:\n",
    "            final_label = \"Unsafe (Phishing - Content Alert!)\"\n",
    "            decision_reason = \"Content-Based Feature Override\"\n",
    "            classification_status = \"Suspicious\"\n",
    "            report_details['Suspicious Activity'] = \", \".join(suspicious_activity_reasons)\n",
    "        elif keyword_score == 0 or login_form_score == 0 or external_link_ratio_score == 0:\n",
    "            if \"Suspicious Activity\" == report_details['Suspicious Activity']: # If not already set by strong signals\n",
    "                report_details['Suspicious Activity'] = \"Some Content Features Neutral/Minor Suspicion\"\n",
    "            classification_status = \"Suspicious\"\n",
    "            if \"Unsafe\" not in final_label: # Don't downgrade if already unsafe\n",
    "                 final_label = \"Neutral (Content Minor Suspicion)\"\n",
    "\n",
    "\n",
    "    # Map final label to risk classification for the report\n",
    "    if \"Unsafe\" in final_label:\n",
    "        report_details['Overall URL risk classification'] = \"Suspicious\"\n",
    "        report_details['Phishing'] = \"Phishing Suspected\"\n",
    "        if report_details['Suspicious Activity'] == 'N/A':\n",
    "            report_details['Suspicious Activity'] = \"Model Indication\"\n",
    "    elif \"Neutral\" in final_label:\n",
    "        report_details['Overall URL risk classification'] = \"Suspicious\"\n",
    "        report_details['Phishing'] = \"Possibly benign, check manually\"\n",
    "        if report_details['Suspicious Activity'] == 'N/A':\n",
    "            report_details['Suspicious Activity'] = \"Neutral Model Indication\"\n",
    "    else: # Safe\n",
    "        report_details['Overall URL risk classification'] = \"Clean URL - SAFE\"\n",
    "        report_details['Phishing'] = \"No Phishing Issues\"\n",
    "        report_details['Suspicious Activity'] = \"No Suspicious Activity\"\n",
    "\n",
    "    # Risk Score: Invert confidence (0-100 scale)\n",
    "    report_details['Risk Score'] = f\"{int((1 - confidence) * 100)} - {report_details['Overall URL risk classification'].split(' - ')[-1]}\"\n",
    "    if report_details['Risk Score'] == \"0 - SAFE\": # Make sure it's not \"0 - Suspicious\"\n",
    "        report_details['Risk Score'] = \"0 - Clean\"\n",
    "\n",
    "    # Placeholder for Malware/Spamming/Parked\n",
    "    report_details['Malware'] = \"No Malware Issues\" # This would need a separate malware scanning integration\n",
    "    report_details['Spamming Domain'] = \"No SPAM Issues\" # This would need a separate spam detection integration\n",
    "    report_details['Parked Domain'] = \"Not Parked\" # This would need more sophisticated detection\n",
    "\n",
    "    return final_label, prob_dist, confidence, decision_reason, report_details\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL_PATH = 'phishing_gradient_boosting_model.joblib'\n",
    "    try:\n",
    "        # Load the model that was trained on 30 features\n",
    "        model_30_features = joblib.load(MODEL_PATH)\n",
    "        # CRITICAL CHECK: Ensure the loaded model truly expects 30 features\n",
    "        expected_features_count = 30 # Assuming your original dataset.csv had exactly 30 feature columns\n",
    "        if model_30_features.n_features_in_ != expected_features_count:\n",
    "            print(f\"Error: Loaded model expects {model_30_features.n_features_in_} features, but this script is designed for a {expected_features_count}-feature model.\")\n",
    "            print(f\"Please ensure '{MODEL_PATH}' was trained using exactly {expected_features_count} features from your dataset.csv (excluding 'Result' and 'index').\")\n",
    "            print(\"You might need to re-run your 'train_and_save_model.py' if it was accidentally trained on a different number of features.\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file '{MODEL_PATH}' not found. Please run your training script ('train_and_save_model.py') first to create it.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Phishing URL Detector is ready. Enter a URL to check.\")\n",
    "    while True:\n",
    "        test_url = input(\"Enter a URL (or type 'exit' to quit): \")\n",
    "        if test_url.lower() == 'exit':\n",
    "            break\n",
    "        if not test_url.startswith(('http://', 'https://')):\n",
    "            test_url = 'https://' + test_url\n",
    "        \n",
    "        final_label, prob_dist, confidence, decision_reason, report_details = predict_url_with_enhancement(\n",
    "            model_30_features, test_url, safe_confidence_threshold=0.98 # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        # --- Print the Detailed Report ---\n",
    "        print(f\"\\nURL Scanning Details for {report_details['Full URL']}\")\n",
    "        print(f\"Overall URL risk classification. {report_details['Overall URL risk classification']}\")\n",
    "        print(f\"Suspicious Activity. {report_details['Suspicious Activity']}\")\n",
    "        print(f\"Domain. {report_details['Domain']}\")\n",
    "        print(f\"IP Address. {report_details['IP Address']}\")\n",
    "        print(f\"Malware. {report_details['Malware']}\")\n",
    "        print(f\"Phishing. {report_details['Phishing']}\")\n",
    "        print(f\"Risk Score. {report_details['Risk Score']}\")\n",
    "        print(f\"Parked Domain. {report_details['Parked Domain']}\")\n",
    "        print(f\"Spamming Domain. {report_details['Spamming Domain']}\")\n",
    "        print(f\"Domain Trust Rating. Login to View\") # Placeholder as before\n",
    "        print(f\"Domain Age. {report_details['Domain Age']}\")\n",
    "        print(f\"HTTP Status Code. {report_details['HTTP Status Code']}\")\n",
    "        print(f\"Page Size. {report_details['Page Size']}\")\n",
    "        print(f\"Web Server. {report_details['Web Server']}\")\n",
    "        print(f\"Content Type. {report_details['Content Type']}\")\n",
    "        print(f\"Free Hosted Content. {report_details['Free Hosted Content']}\")\n",
    "        print(f\"SPF/DMARC Record. {report_details['SPF/DMARC Record']}\")\n",
    "        print(f\"DNS A Records. {report_details['DNS A Records']}\")\n",
    "        \n",
    "        print(\"\\n--- Model's Internal Details ---\")\n",
    "        print(f\"Model's Final Decision: {final_label}\")\n",
    "        print(f\"Decision Based On: {decision_reason}\")\n",
    "        print(f\"Model's Highest Confidence (from 30 features): {confidence:.2%}\")\n",
    "        print(\"Model's Individual Class Probabilities (from 30 features):\")\n",
    "        for class_val in sorted(prob_dist.keys()):\n",
    "            class_label_map = {1: \"Safe\", 0: \"Neutral\", -1: \"Unsafe (Phishing)\"}\n",
    "            print(f\"  {prob_dist[class_val]:.1%} chance of being {class_val} ({class_label_map.get(class_val, 'Unknown')})\")\n",
    "        print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc218cd-e4ab-4144-870e-a5b25839ba90",
   "metadata": {},
   "source": [
    "# Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85c8bac-29cc-4418-945d-06411d5079de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Starting evaluation on 30 random URLs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c165361fa4469abe915b6845c21446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance Report on Sampled Dataset ---\n",
      "ðŸŽ¯ Overall Accuracy on 30 URLs: 60.00%\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.56      0.71      0.62        14\n",
      "   malicious       0.67      0.50      0.57        16\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.61      0.61      0.60        30\n",
      "weighted avg       0.61      0.60      0.60        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import sys\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter!\n",
    "import whois\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import dns.resolver\n",
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# === 1. CONFIGURE YOUR FILE DETAILS AND SAMPLE SIZE HERE ===\n",
    "# ==============================================================================\n",
    "FILE_CONFIG = {\n",
    "    \"path\": \"balanced_urls.csv\",      # <--- CHANGE THIS to your test file name\n",
    "    \"url_column\": \"url\",              # <--- CHANGE THIS to the name of the URL column\n",
    "    \"label_column\": \"type\",           # <--- CHANGE THIS to the name of the label column\n",
    "    \"benign_label_value\": \"benign\"      # <--- CHANGE THIS to the value for a safe URL\n",
    "}\n",
    "\n",
    "# Set the number of random URLs you want to test\n",
    "SAMPLE_SIZE = 30 # <--- YOU CAN CHANGE THIS NUMBER\n",
    "# ==============================================================================\n",
    "# === 2. ALL FEATURE EXTRACTION FUNCTIONS (Self-Contained) ===\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_domain(url):\n",
    "    try: return urllib.parse.urlparse(url).netloc\n",
    "    except: return None\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# --- Feature Functions ---\n",
    "def having_ip_address(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        if re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\", domain): return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def url_length(url):\n",
    "    if len(url) < 54: return 1\n",
    "    elif 54 <= len(url) <= 75: return 0\n",
    "    else: return -1\n",
    "\n",
    "def shortening_service(url):\n",
    "    shortening_services = [\"bit.ly\", \"goo.gl\", \"t.co\", \"tinyurl.com\"]\n",
    "    domain = get_domain(url)\n",
    "    if domain in shortening_services: return -1\n",
    "    return 1\n",
    "\n",
    "def having_at_symbol(url):\n",
    "    if \"@\" in url: return -1\n",
    "    return 1\n",
    "\n",
    "def double_slash_redirecting(url):\n",
    "    if \"//\" in urllib.parse.urlparse(url).path: return -1\n",
    "    return 1\n",
    "\n",
    "def prefix_suffix(url):\n",
    "    if \"-\" in get_domain(url): return -1\n",
    "    return 1\n",
    "\n",
    "def having_sub_domain(url):\n",
    "    dots = get_domain(url).count('.')\n",
    "    if dots == 2: return 0\n",
    "    elif dots > 2: return -1\n",
    "    return 1\n",
    "\n",
    "def ssl_final_state(url):\n",
    "    try:\n",
    "        if url.startswith(\"https\"): return 1\n",
    "        return -1\n",
    "    except: return -1\n",
    "\n",
    "def domain_registration_length(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if w.expiration_date:\n",
    "            exp_date = w.expiration_date[0] if isinstance(w.expiration_date, list) else w.expiration_date\n",
    "            cre_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
    "            if exp_date and cre_date:\n",
    "                if (exp_date - cre_date).days / 365 <= 1: return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def age_of_domain(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if w.creation_date:\n",
    "            cre_date = w.creation_date[0] if isinstance(w.creation_date, list) else w.creation_date\n",
    "            if (datetime.now() - cre_date).days < 180: return -1\n",
    "        return 1\n",
    "    except: return -1\n",
    "\n",
    "def dns_record(url):\n",
    "    try:\n",
    "        dns.resolver.resolve(get_domain(url), 'A'); return 1\n",
    "    except: return -1\n",
    "\n",
    "def abnormal_url(url):\n",
    "    try:\n",
    "        domain = get_domain(url)\n",
    "        w = whois.whois(domain)\n",
    "        if domain.lower() not in str(w).lower():\n",
    "            return -1\n",
    "        return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# Placeholders for complex features\n",
    "def web_traffic(url): return 0\n",
    "def page_rank(url): return 0\n",
    "def google_index(url): return 1\n",
    "def links_pointing_to_page(url): return 0\n",
    "def statistical_report(url): return 1\n",
    "def favicon(url): return 1\n",
    "def port(url): return 1\n",
    "def https_token(url): return 1\n",
    "def request_url(url): return 1\n",
    "def url_of_anchor(url): return 1\n",
    "def links_in_tags(url): return 1\n",
    "def sfh(url): return 1\n",
    "def submitting_to_email(url): return 1\n",
    "def redirect(url): return 1\n",
    "def on_mouseover(url): return 1\n",
    "def right_click(url): return 1\n",
    "def popup_window(url): return 1\n",
    "def iframe(url): return 1\n",
    "\n",
    "# --- Prediction Wrapper Function ---\n",
    "def predict_url(model, url):\n",
    "    if dns_record(url) == -1:\n",
    "        return \"Unsafe (Phishing)\", 1.0\n",
    "\n",
    "    features = [\n",
    "        having_ip_address(url), url_length(url), shortening_service(url),\n",
    "        having_at_symbol(url), double_slash_redirecting(url), prefix_suffix(url),\n",
    "        having_sub_domain(url), ssl_final_state(url), domain_registration_length(url),\n",
    "        favicon(url), port(url), https_token(url), request_url(url), url_of_anchor(url),\n",
    "        links_in_tags(url), sfh(url), submitting_to_email(url), abnormal_url(url),\n",
    "        redirect(url), on_mouseover(url), right_click(url), popup_window(url),\n",
    "        iframe(url), age_of_domain(url), dns_record(url), web_traffic(url),\n",
    "        page_rank(url), google_index(url), links_pointing_to_page(url),\n",
    "        statistical_report(url)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        prediction = model.predict([features])[0]\n",
    "        label_map = {1: \"Safe\", 0: \"Neutral\", -1: \"Unsafe (Phishing)\"}\n",
    "        label = label_map.get(prediction, \"Unknown\")\n",
    "        return label, 0.0 # Confidence isn't used in this script, return dummy value\n",
    "    except Exception as e:\n",
    "        return f\"Error\", 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# === 3. MAIN EVALUATION LOGIC ===\n",
    "# ==============================================================================\n",
    "def evaluate_on_sample(model_path='phishing_gradient_boosting_model.joblib'):\n",
    "    # Load model and dataset\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Model file '{model_path}' not found in the same directory as your notebook.\")\n",
    "        return # Use return instead of sys.exit in notebooks\n",
    "\n",
    "    try:\n",
    "        df_test = pd.read_csv(FILE_CONFIG[\"path\"])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Test dataset '{FILE_CONFIG['path']}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Take random sample\n",
    "    if len(df_test) > SAMPLE_SIZE:\n",
    "        df_sample = df_test.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    else:\n",
    "        df_sample = df_test\n",
    "    \n",
    "    print(f\"âœ… Starting evaluation on {len(df_sample)} random URLs...\")\n",
    "    \n",
    "    true_labels = []\n",
    "    mapped_predictions = []\n",
    "\n",
    "    # Iterate, predict, and map\n",
    "    for index, row in tqdm(df_sample.iterrows(), total=df_sample.shape[0]):\n",
    "        url = row[FILE_CONFIG[\"url_column\"]]\n",
    "        true_label_text = row[FILE_CONFIG[\"label_column\"]]\n",
    "        \n",
    "        model_label, _ = predict_url(model, url)\n",
    "        \n",
    "        # Apply the conservative mapping rule\n",
    "        if model_label == \"Safe\":\n",
    "            mapped_prediction_text = FILE_CONFIG[\"benign_label_value\"]\n",
    "        else: # Treat \"Neutral\" and \"Unsafe\" as malicious\n",
    "            # Determine the opposite of the benign label\n",
    "            malicious_label_value = \"malicious\" # A sensible default\n",
    "            if true_label_text != FILE_CONFIG[\"benign_label_value\"]:\n",
    "                 malicious_label_value = true_label_text\n",
    "            mapped_prediction_text = malicious_label_value\n",
    "            \n",
    "        mapped_predictions.append(mapped_prediction_text)\n",
    "        true_labels.append(true_label_text)\n",
    "\n",
    "    # Calculate and report accuracy\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    \n",
    "    print(\"\\n--- Model Performance Report on Sampled Dataset ---\")\n",
    "    accuracy = accuracy_score(true_labels, mapped_predictions)\n",
    "    print(f\"ðŸŽ¯ Overall Accuracy on {len(df_sample)} URLs: {accuracy:.2%}\\n\")\n",
    "    \n",
    "    print(\"ðŸ“Š Classification Report:\")\n",
    "    print(classification_report(true_labels, mapped_predictions))\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "evaluate_on_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3f8d91-9356-4820-8452-3ad6bcd3a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b774ed4b-d77f-4eb6-89b6-5359a7667256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>having_IPhaving_IP_Address</th>\n",
       "      <th>URLURL_Length</th>\n",
       "      <th>Shortining_Service</th>\n",
       "      <th>having_At_Symbol</th>\n",
       "      <th>double_slash_redirecting</th>\n",
       "      <th>Prefix_Suffix</th>\n",
       "      <th>having_Sub_Domain</th>\n",
       "      <th>SSLfinal_State</th>\n",
       "      <th>Domain_registeration_length</th>\n",
       "      <th>...</th>\n",
       "      <th>popUpWidnow</th>\n",
       "      <th>Iframe</th>\n",
       "      <th>age_of_domain</th>\n",
       "      <th>DNSRecord</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>Page_Rank</th>\n",
       "      <th>Google_Index</th>\n",
       "      <th>Links_pointing_to_page</th>\n",
       "      <th>Statistical_report</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  having_IPhaving_IP_Address  URLURL_Length  Shortining_Service  \\\n",
       "0      1                          -1              1                   1   \n",
       "1      2                           1              1                   1   \n",
       "2      3                           1              0                   1   \n",
       "3      4                           1              0                   1   \n",
       "4      5                           1              0                  -1   \n",
       "\n",
       "   having_At_Symbol  double_slash_redirecting  Prefix_Suffix  \\\n",
       "0                 1                        -1             -1   \n",
       "1                 1                         1             -1   \n",
       "2                 1                         1             -1   \n",
       "3                 1                         1             -1   \n",
       "4                 1                         1             -1   \n",
       "\n",
       "   having_Sub_Domain  SSLfinal_State  Domain_registeration_length  ...  \\\n",
       "0                 -1              -1                           -1  ...   \n",
       "1                  0               1                           -1  ...   \n",
       "2                 -1              -1                           -1  ...   \n",
       "3                 -1              -1                            1  ...   \n",
       "4                  1               1                           -1  ...   \n",
       "\n",
       "   popUpWidnow  Iframe  age_of_domain  DNSRecord  web_traffic  Page_Rank  \\\n",
       "0            1       1             -1         -1           -1         -1   \n",
       "1            1       1             -1         -1            0         -1   \n",
       "2            1       1              1         -1            1         -1   \n",
       "3            1       1             -1         -1            1         -1   \n",
       "4           -1       1             -1         -1            0         -1   \n",
       "\n",
       "   Google_Index  Links_pointing_to_page  Statistical_report  Result  \n",
       "0             1                       1                  -1      -1  \n",
       "1             1                       1                   1      -1  \n",
       "2             1                       0                  -1      -1  \n",
       "3             1                      -1                   1      -1  \n",
       "4             1                       1                   1       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b6c44b-1b76-4ce5-b868-dc2665e8160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'having_IPhaving_IP_Address', 'URLURL_Length',\n",
       "       'Shortining_Service', 'having_At_Symbol', 'double_slash_redirecting',\n",
       "       'Prefix_Suffix', 'having_Sub_Domain', 'SSLfinal_State',\n",
       "       'Domain_registeration_length', 'Favicon', 'port', 'HTTPS_token',\n",
       "       'Request_URL', 'URL_of_Anchor', 'Links_in_tags', 'SFH',\n",
       "       'Submitting_to_email', 'Abnormal_URL', 'Redirect', 'on_mouseover',\n",
       "       'RightClick', 'popUpWidnow', 'Iframe', 'age_of_domain', 'DNSRecord',\n",
       "       'web_traffic', 'Page_Rank', 'Google_Index', 'Links_pointing_to_page',\n",
       "       'Statistical_report', 'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759aa836-e01b-4539-8b55-0f4c70dd20f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
